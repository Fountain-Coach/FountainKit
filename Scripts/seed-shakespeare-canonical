#!/usr/bin/env python3
import argparse, json, os, sys, subprocess
from pathlib import Path
from urllib.parse import urlparse

CANON_VERSION = 'v1'

def http_json(method, url, obj, timeout=60):
    data = json.dumps(obj, ensure_ascii=False).encode('utf-8')
    out = subprocess.check_output([
        'curl','-sf','-X',method, url,
        '-H','Content-Type: application/json',
        '--data-binary','@-'
    ], input=data, timeout=timeout)
    return json.loads(out.decode('utf-8'))

def post_json(url, obj):
    return http_json('POST', url, obj)

def get_json(url, timeout=30):
    out = subprocess.check_output(['curl','-sf',url], timeout=timeout)
    return json.loads(out.decode('utf-8'))

def pick_source(canon_list):
    # Prefer MIT when present
    mit = next((c for c in canon_list if c.get('name','').lower().find('mit') >= 0), None)
    if mit: return mit
    return canon_list[0] if canon_list else None

def main():
    ap = argparse.ArgumentParser(description='Seed canonical Shakespeare edition into FountainStore via Semantic Browser analysis + Persist API')
    ap.add_argument('--mapping', default='Configuration/shakespeare-canonical.json')
    ap.add_argument('--sb-url', default=os.environ.get('SEMANTIC_BROWSER_URL','http://127.0.0.1:8007'))
    ap.add_argument('--persist-url', default=os.environ.get('PERSIST_URL','http://127.0.0.1:8005'))
    ap.add_argument('--corpus', default='shakespeare-canonical')
    ap.add_argument('--limit', type=int, default=0, help='Optional limit for quick runs')
    ap.add_argument('--version', default=CANON_VERSION, help='Canonical seed version (bump to create new baseline + drift)')
    args = ap.parse_args()

    root = Path.cwd()
    mapping = json.loads((root/args.mapping).read_text(encoding='utf-8'))
    items = mapping[:args.limit] if args.limit and args.limit > 0 else mapping

    # Ensure corpus exists
    try:
        post_json(f"{args.persist_url.rstrip('/')}/corpora", {"corpusId": args.corpus})
    except subprocess.CalledProcessError:
        pass

    # Guard: if a canonical baseline with same version exists, skip seeding
    prior_mapping = None
    prior_version = None
    try:
        bl = get_json(f"{args.persist_url.rstrip('/')}/corpora/{args.corpus}/baselines?limit=200")
        for item in bl.get('baselines', []) or []:
            content = item.get('content') or item.get('value2',{}).get('content')
            if not content:
                continue
            try:
                obj = json.loads(content)
            except Exception:
                continue
            if obj.get('kind') == 'canonical-shakespeare-seed':
                # Track last seen canonical baseline for drift
                prior_mapping = obj.get('mapping')
                prior_version = obj.get('version')
            if obj.get('kind') == 'canonical-shakespeare-seed' and obj.get('version') == args.version:
                print(f"[seed] Guard: canonical version {CANON_VERSION} already present. Skipping seeding.")
                return
    except Exception:
        # Best-effort guard; continue if fetch fails
        pass

    for it in items:
        title = it['title']
        canon = pick_source(it.get('canonical',[]))
        if not canon:
            print(f"[seed] WARN: no canonical source for {title}", file=sys.stderr)
            continue
        url = canon['url']
        print(f"[seed] {title} ← {url}")
        # 1) Ask Semantic Browser to browse + analyze
        browse_req = {
            "url": url,
            "wait": {"strategy": "networkIdle", "networkIdleMs": 500, "maxWaitMs": 15000},
            "mode": "standard"
        }
        resp = post_json(f"{args.sb_url.rstrip('/')}/v1/browse", browse_req)
        analysis = resp.get('analysis')
        if not analysis:
            print(f"[seed] ERROR: no analysis for {title}", file=sys.stderr)
            continue
        env = analysis.get('envelope', {})
        slug = it.get('slug')
        page_id = slug or env.get('id')
        page_url = env.get('source',{}).get('uri') or url
        host = urlparse(page_url).netloc or 'canonical'
        # Title: prefer mapping title; else first heading block
        blocks = analysis.get('blocks', [])
        heading = next((b.get('text') for b in blocks if (b.get('kind') or '').lower() == 'heading'), None)
        page_title = title or heading or host
        # 2) Upsert Page
        page = {"corpusId": args.corpus, "pageId": page_id, "url": page_url, "host": host, "title": page_title}
        post_json(f"{args.persist_url.rstrip('/')}/corpora/{args.corpus}/pages", page)
        # 3) Upsert Segments (all blocks as paragraph/heading/code/caption/table JSON)
        for b in blocks:
            seg_id = b.get('id')
            kind = (b.get('kind') or 'paragraph')
            text = b.get('text') or ''
            # Serialise tables inline as JSON lines when present
            if not text and b.get('table') is not None:
                text = json.dumps(b['table'], ensure_ascii=False)
            if not seg_id:
                continue
            seg = {"corpusId": args.corpus, "segmentId": f"{page_id}:{seg_id}", "pageId": page_id, "kind": kind, "text": text}
            try:
                post_json(f"{args.persist_url.rstrip('/')}/corpora/{args.corpus}/segments", seg)
            except subprocess.CalledProcessError:
                # Skip on collision
                pass
        # 4) Add an analysis summary line
        summ = analysis.get('summaries',{})
        abstract = summ.get('abstract') or ''
        summary_line = f"seeded canonical from {host} {url}"
        if abstract:
            summary_line = abstract[:200]
        try:
            post_json(f"{args.persist_url.rstrip('/')}/corpora/{args.corpus}/analyses", {"corpusId": args.corpus, "analysisId": f"seed:{page_id}", "pageId": page_id, "summary": summary_line})
        except subprocess.CalledProcessError:
            pass

    # Drift detection vs prior baseline
    try:
        def primary_url(m):
            if not m: return None
            # Prefer MIT
            mit = next((c for c in m if 'mit' in (c.get('name','').lower() or c.get('url',''))), None)
            return (mit or m[0]).get('url')
        added = []
        removed = []
        changed = []
        if prior_mapping:
            old = {d['title']: primary_url(d.get('canonical',[])) for d in prior_mapping}
            new = {d['title']: primary_url(d.get('canonical',[])) for d in mapping}
            for t in new.keys() - old.keys(): added.append(t)
            for t in old.keys() - new.keys(): removed.append(t)
            for t in new.keys() & old.keys():
                if (old.get(t) or '') != (new.get(t) or ''):
                    changed.append({'title': t, 'from': old.get(t), 'to': new.get(t)})
        # Totals
        import urllib.request
        pages_total = get_json(f"{args.persist_url.rstrip('/')}/corpora/{args.corpus}/pages?limit=1").get('total', 0)
        segs_total = get_json(f"{args.persist_url.rstrip('/')}/corpora/{args.corpus}/segments?limit=1").get('total', 0)
        # Post drift as baseline
        import datetime
        ts = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
        commit = subprocess.check_output(['git','rev-parse','--short','HEAD']).decode().strip()
        drift = {
            'kind': 'canonical-shakespeare-drift',
            'fromVersion': prior_version,
            'toVersion': args.version,
            'addedTitles': added,
            'removedTitles': removed,
            'changedSources': changed,
            'totals': {'pages': pages_total, 'segments': segs_total},
            'seed': {'by': 'Scripts/seed-shakespeare-canonical', 'commit': commit, 'time': ts}
        }
        drift_id = f"canonical-drift-{args.version}-{ts}-{commit}"
        post_json(f"{args.persist_url.rstrip('/')}/corpora/{args.corpus}/baselines", {
            'corpusId': args.corpus,
            'baselineId': drift_id,
            'content': json.dumps(drift, ensure_ascii=False)
        })
        # And a short analysis summary
        summary_line = f"drift: +{len(added)} / -{len(removed)} / ~{len(changed)} (pages={pages_total}, segments={segs_total})"
        post_json(f"{args.persist_url.rstrip('/')}/corpora/{args.corpus}/analyses", {
            'corpusId': args.corpus,
            'analysisId': f"drift:{prior_version or 'nil'}→{args.version}",
            'pageId': 'canonical-index',
            'summary': summary_line
        })
    except Exception as e:
        print(f"[seed] WARN: drift report failed: {e}", file=sys.stderr)

    print(f"[seed] Completed seeding corpus '{args.corpus}'")

if __name__ == '__main__':
    main()
